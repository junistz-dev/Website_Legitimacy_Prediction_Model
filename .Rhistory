# the NA value in column [11:24] will be median of their value
median_val <- median(column, na.rm = TRUE)
column[is.na(column)] <- median_val
} else if (i %in% 33:38) {
# the NA value in column [33:38] will be median of their value
median_val <- median(column, na.rm = TRUE)
column[is.na(column)] <- median_val
} else if (i %in% 39:44) {
# the NA value in column [39:44] will be 0
column[is.na(column)] <- 0
} else if (i %in% 44:47) {
# the NA value in column [45:47] will be median of their value
median_val <- median(column, na.rm = TRUE)
column[is.na(column)] <- median_val
} else if (i %in% 49:52) {
# the NA value in column [49:52] will be median of their value
median_val <- median(column, na.rm = TRUE)
column[is.na(column)] <- median_val
}
data[[i]] <- column
}
return(data)
}
# Pre-processing done for int
cvbase_updated <- preprocessed_data(cvbase_filter_country)
# Change Chr(A~F) to 1~6 int
cvbase_updated[27:32] <- lapply(cvbase_updated[27:32], function(x) ifelse(is.na(x), NA, match(x, LETTERS)))
preprocessed_data2 <- function(data) {
# make it NA to median which column is [27:32]
data[, 27:32] <- lapply(data[, 27:32], function(x) {
median_val <- median(x, na.rm = TRUE)
replace(x, is.na(x), median_val)
})
return(data)
}
cvbase_final <- preprocessed_data2(cvbase_updated)
# make copy of pre-processed data
Q2_sample <- cvbase_final
# extract data for selected country from Q2_sample
Q2_sample_for_italy = Q2_sample %>% filter(Q2_sample$coded_country == 'Italy')
Q2_sample_for_italy_save = Q2_sample_for_italy
# extract data for non-selected country from Q2 sample
Q2_sample_not_italy = Q2_sample %>% filter(Q2_sample$coded_country != 'Italy')
Q2_sample_not_italy_save = Q2_sample_not_italy
# Make all data type to integer except Coded_Country
Q2_sample_for_italy <- Q2_sample_for_italy %>%
mutate_if(function(x) !is.character(x), as.integer)
# subset non-integer data for both table
Q2_sample_for_italy <- subset(Q2_sample_for_italy, select = -coded_country)
Q2_sample_not_italy <- subset(Q2_sample_not_italy, select = -coded_country)
# make average data table for Question 2
mean_data_italy <- colMeans(Q2_sample_for_italy)
mean_data_non_italy <- colMeans(Q2_sample_not_italy)
Q2_a_result <- t.test(Q2_sample_for_italy,Q2_sample_not_italy)
Q2_a_result
# trying to see difference between both table set.
mean_diff <- mean_data_italy - mean_data_non_italy
diff_table <- data.frame(
Attribute = names(mean_diff),
Mean_Difference = mean_diff)
diff_table$Attribute <- factor(diff_table$Attribute, levels = unique(diff_table$Attribute))
# ggplot2 graph
comparison_plot <- ggplot(diff_table, aes(x = Attribute, y = Mean_Difference)) +
geom_bar(stat = "identity", fill = "skyblue", width = 0.5) +
labs(title = "Difference in Mean Values between Italy and Non-Italy",
x = "Attribute",
y = "Mean Difference (italy - non italy)") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1), plot.title = element_text(hjust = 0.5))
comparison_plot
c19_italy <-colMeans(Q2_sample_for_italy[48:51])
italy_table <- data.frame(
Attribute = names(c19_italy),
Mean_value = c19_italy)
Q2_b_table <- ggplot(italy_table, aes(x = Attribute, y = Mean_value)) +
geom_bar(stat = "identity", fill = "grey", width = 0.5) +
geom_text(aes(label = sprintf("%.2f", Mean_value)), vjust = -0.5, size = 3) +
labs(title = "Average Value for Corona ProSocial Behavior in Italy",
x = "Attribute", y = "Value") +
theme_minimal() +
theme(axis.text.x = element_text(), plot.title = element_text(hjust = 0.5))
Q2_b_table
# 4 of c19 columns
Q2_b_italy_c19 <- Q2_sample_for_italy[48:51]
# whole responses
Q2_b_italy_response <- Q2_sample_for_italy[1:47]
# getting the cor relationship between c19 and responses
Q2_italy_cor <- cor(Q2_b_italy_response, Q2_b_italy_c19)
# make the empty data frame for 2 a
top_5_values_df <- data.frame(Attribute = character(), Value = numeric(), stringsAsFactors = FALSE)
# find the attribute that has biggest value of attribute for cor relationship,
# each row 7 attributes
for (col_index in 1:ncol(Q2_italy_cor)) {
col_values <- Q2_italy_cor[, col_index]
abs_col_values <- abs(col_values)
top_5_idx <- order(abs_col_values, decreasing = TRUE)[1:7]
top_5_values <- col_values[top_5_idx]
top_5_attribute_names <- rownames(Q2_italy_cor)[top_5_idx]
# save 7 for each row, exclude same attributes
for (i in 1:7) {
if (!(top_5_attribute_names[i] %in% top_5_values_df$Attribute)) {
top_5_values_df <- rbind(top_5_values_df, data.frame(Attribute = top_5_attribute_names[i], Value = top_5_values[i]))
}
}
}
# sort
top_5_values_df <- top_5_values_df[order(-abs(top_5_values_df$Value)),]
top_5_values_df
# pro_social_col 정의
Q2_B_proso_for_italy <- colnames(Q2_sample_for_italy)[48:51]
# pro_social 데이터 추출
pro_social_for_italy <- Q2_sample_for_italy[, Q2_B_proso_for_italy]
# select which attributes is selected in data
selected_attributes <- top_5_values_df$Attribute
# calculate the correlation between data and pro-social attributes
corr_matrix_for_italy <- cor(Q2_sample_for_italy[, selected_attributes], pro_social_for_italy)
# reframe the dataset for heatmap
corr_melt_for_italy <- melt(corr_matrix_for_italy)
# heatmap
heatmap_sa_for_italy <- ggplot(corr_melt_for_italy, aes(Var2, Var1)) +
geom_tile(aes(fill = value)) +
scale_fill_gradient2(low = "pink", high ="lightblue", mid = "white", midpoint = 0) +
theme(axis.text.x = element_text(size = 8),  # x 축의 레이블 크기 및 각도 조정
axis.text.y = element_text(size = 5),  # y 축의 레이블 크기 조정
plot.title = element_text(size = 12, hjust = 0.5)) +  # 그래프 제목 크기 조정
labs(title = "Correlation Heatmap for Italy - Question 2B") +
xlab("pro-social attitudes") +
ylab("participant responses in Italy")
# heatmap
heatmap_sa_for_italy
# Progression for making attribute format for lm()
predictors_formula_other <- paste(top_5_values_df$Attribute, collapse = " + ")
Q2_italy_so1 <- lm(paste("c19ProSo01 ~", predictors_formula_other, "+ c19ProSo02 + c19ProSo03 + c19ProSo04"), data = Q2_sample_for_italy)
Q2_italy_so2 <- lm(paste("c19ProSo02 ~", predictors_formula_other, "+ c19ProSo01 + c19ProSo03 + c19ProSo04"), data = Q2_sample_for_italy)
Q2_italy_so3 <- lm(paste("c19ProSo03 ~", predictors_formula_other, "+ c19ProSo01 + c19ProSo02 + c19ProSo04"), data = Q2_sample_for_italy)
Q2_italy_so4 <- lm(paste("c19ProSo04 ~", predictors_formula_other, "+ c19ProSo01 + c19ProSo02 + c19ProSo03"), data = Q2_sample_for_italy)
# Output
summary(Q2_italy_so1)
summary(Q2_italy_so2)
summary(Q2_italy_so3)
summary(Q2_italy_so4)
# C19 for Non-Italy
c19_not_italy <-colMeans(Q2_sample_not_italy[48:51])
# Making Non-Italy Data Frame
not_italy_table <- data.frame(
Attribute = names(c19_not_italy),
Mean_value = c19_not_italy)
# ggplot
q2_c_table <- ggplot(not_italy_table, aes(x = Attribute, y = Mean_value)) +
geom_bar(stat = "identity", fill = "grey", width = 0.5) +
geom_text(aes(label = sprintf("%.2f", Mean_value)), vjust = -0.5, size = 3) +
labs(title = "Average Value for Corona ProSocial Behavior (Non-Italy)",
x = "Attribute", y = "Value") +
theme_minimal() +
theme(axis.text.x = element_text(), plot.title = element_text(hjust = 0.5))
q2_c_table
# C19, 4 columns for Non-Italy Q2-c
Q2_c_other_c19 <- Q2_sample_not_italy[48:51]
# Responses from Non-Italy people
Q2_c_other_response <- Q2_sample_not_italy[1:47]
# Co-relationship for Non-Italy
Q2_non_italy_cor <- cor(Q2_c_other_response,Q2_c_other_c19)
# Making the Data Frame
top_5_values_df_other <- data.frame(Attribute = character(), Value = numeric(), stringsAsFactors = FALSE)
# find the attribute that has biggest value of attribute for cor relationship,
# each row 7 attributes
for (col_index in 1:ncol(Q2_non_italy_cor)) {
col_values_other <- Q2_non_italy_cor[, col_index]
abs_col_values <- abs(col_values_other)
top_5_idx_other <- order(abs_col_values, decreasing = TRUE)[1:7]
top_5_values_other <- col_values_other[top_5_idx_other]
top_5_attribute_names_other <- rownames(Q2_non_italy_cor)[top_5_idx_other]
# save 7 for each row, exclude same attributes
for (i in 1:7) {
if (!(top_5_attribute_names_other[i] %in% top_5_values_df_other$Attribute)) {
top_5_values_df_other <- rbind(top_5_values_df_other, data.frame(Attribute = top_5_attribute_names_other[i], Value = top_5_values_other[i]))
}
}
}
# sort
top_5_values_df_other <- top_5_values_df_other[order(-abs(top_5_values_df_other$Value)),]
top_5_values_df_other
# trying to get column name of prosocial attribute
Q2_C_proso_not_italy <- colnames(Q2_sample_not_italy)[48:51]
# extract prosocial data
pro_social_not_italy <- Q2_sample_not_italy[, Q2_C_proso_not_italy]
# extract chosen attribute
selected_attributes_other <- top_5_values_df_other$Attribute
# find the correlation between prosocial attribute value and data
corr_matrix_not_italy <- cor(Q2_sample_not_italy[, selected_attributes_other], pro_social_not_italy)
# reframe the data for heatmap
corr_melt_not_italy <- melt(corr_matrix_not_italy)
# heatmap
heatmap_sa_not_italy <- ggplot(corr_melt_not_italy, aes(Var2, Var1)) +
geom_tile(aes(fill = value)) +
scale_fill_gradient2(low = "pink", high ="lightblue", mid = "white", midpoint = 0) +
theme(axis.text.x = element_text(size = 8),
axis.text.y = element_text(size = 5),
plot.title = element_text(size = 12, hjust = 0.5)) +
labs(title = "Correlation Heatmap not Italy - Question 2C") +
xlab("pro-social attitudes") +
ylab("participant responses in Not - Italy")
heatmap_sa_not_italy
# Progression for making attribute format for lm()
predictors_formula_other <- paste(top_5_values_df_other$Attribute, collapse = " + ")
Q2_other_so1 <- lm(paste("c19ProSo01 ~", predictors_formula_other, "+ c19ProSo02 + c19ProSo03 + c19ProSo04"), data = Q2_sample_not_italy)
Q2_other_so2 <- lm(paste("c19ProSo02 ~", predictors_formula_other, "+ c19ProSo01 + c19ProSo03 + c19ProSo04"), data = Q2_sample_not_italy)
Q2_other_so3 <- lm(paste("c19ProSo03 ~", predictors_formula_other, "+ c19ProSo01 + c19ProSo02 + c19ProSo04"), data = Q2_sample_not_italy)
Q2_other_so4 <- lm(paste("c19ProSo04 ~", predictors_formula_other, "+ c19ProSo01 + c19ProSo02 + c19ProSo03"), data = Q2_sample_not_italy)
# Output
summary(Q2_other_so1)
summary(Q2_other_so2)
summary(Q2_other_so3)
summary(Q2_other_so4)
# Politics part : Government Performance Score
politics <- read_excel("DataforPolitics.xlsx", sheet = "GovernmentEffectiveness")
politicsd <- data.frame(politics$Data,politics[75])
names(politicsd)[names(politicsd) == "politics.Data"] <- "coded_country"
names(politicsd)[names(politicsd) == "X2011...75"] <- "Political_score"
# exclude the data if that data has N/Ass
politicsd <- politicsd[2:nrow(politicsd), ]
# exclude the data if that data has N/A
politicsd[politicsd == "#N/A"] <- NA
politicsd <- na.omit(politicsd)
politicsd$Political_score <- as.numeric(politicsd$Political_score)
# Vaccinated Rate + GDP per person Data Extracting.
covid_data <- read.csv("DataforCovid.csv")
# filter for 2021 years data
covid_data <- covid_data %>%
filter(substr(as.character(date), 1, 4) == "2021")
# group by based on coded_country
vaccinated <- covid_data %>%
group_by(covid_data$location) %>%
summarize(Average_vaccinated = mean(total_vaccinations_per_hundred, na.rm = TRUE))
# change name to row name to coded_country
names(vaccinated)[names(vaccinated) == "covid_data$location"] <- "coded_country"
# GDP
gdpd <- covid_data %>%
group_by(covid_data$location) %>%
summarize(gdp_per_person = mean(gdp_per_capita, na.rm=TRUE))
names(gdpd)[names(gdpd) == "covid_data$location"] <- "coded_country"
# HAPPY data set
happy <- read_excel("DataForHappy.xls")
# extracting ladder score from happy data
happyd <- data.frame(
Country_name = happy$`Country name`,
Ladder_score = happy$`Ladder score`
)
# change name to coded_country for merge in the future
names(happyd)[names(happyd) == "Country_name"] <- "coded_country"
# Average death rate per week
health <- read.csv("DataforHealth.csv")
# filtering 2021 years only
health_data <- health %>%
filter(Year == 2021)
# group by coded country
health_datad <- health_data %>%
group_by(health_data$Country) %>%
summarize(Average_deaths_per_week = mean(Value, na.rm = TRUE))
# change rowname to coded_country
names(health_datad)[names(health_datad) == "health_data$Country"] <- "coded_country"
# Short - Term labour rate
labour <- read.csv("DataForLabour.csv")
# filtering 2021 year
labour_data <- labour %>%
filter(Time == 2021)
# group by coded country
labour_datad <- labour_data %>%
group_by(labour_data$Country) %>%
summarize(Average_labour_rate = mean(Value, na.rm = TRUE))
# change rowname to coded_country
names(labour_datad)[names(labour_datad) == "labour_data$Country"] <- "coded_country"
# politicsd, vaccinated, happyd, health_datad, labour_datad >> merge
merged_data <- left_join(politicsd, vaccinated, by = "coded_country") %>%
left_join(gdpd, by = "coded_country") %>%
left_join(happyd, by = "coded_country") %>%
left_join(health_datad, by = "coded_country") %>%
left_join(labour_datad, by = "coded_country")
write.csv(merged_data, "June Merged Data.csv", row.names = TRUE)
Q3_a_data <- merged_data
# exclude the data if that data has N/A
Q3_a_data <- na.omit(Q3_a_data)
# make it numeric
Q3_a_data[,2:7] <- lapply(Q3_a_data[,2:7], as.numeric)
# normalization
Q3_a_data[,2:7] <- scale(Q3_a_data[,2:7])
# progression of getting silhouette score
i_silhouette_score <- function(k){
km <- kmeans(Q3_a_data[,2:7], centers = k, nstart=25)
ss <- silhouette(km$cluster, dist(Q3_a_data[,2:7]))
mean(ss[,3])
}
k <- 2:10
avg_sil <- sapply(k,i_silhouette_score)
plot(k,type='b',avg_sil,xlab='Number of Clusters', ylab='Average Silhouette Scores', main=("Silhouette Score for Merged Data"))
ikfit <- kmeans(Q3_a_data[,2:7],3,nstart = 20)
table(actual = Q3_a_data$coded_country, fitted = ikfit$cluster)
ikfit$cluster =as.factor(ikfit$cluster)
cb <- Q3_a_data
cb[,2:7] = scale(cb[2:7])
rownames(cb) = cb$coded_country
cfit = hclust(dist(cb[,2:7]), "average")
plot(cfit,hang=-1)
cut.cfit = cutree(cfit,k=3)
# 클러스터링 결과를 시각화하기 위해 plot 함수를 사용합니다.
plot(cfit, main = "Dendrogram of Clustering", xlab = "Countries")
# 클러스터를 강조하기 위해 사각형 그리기
rect.hclust(cfit, k=3, border = "red")
# final merging with merged_data (extracted from myself) and finalized cvbase
Q3_b_data <- left_join(cvbase_final, merged_data, by = "coded_country")
Q3_b_data <- na.omit(Q3_b_data)
clustered_countries <- c('Italy', 'Chile', 'Colombia', 'Costa Rica', 'Greece', 'Hungary', 'Latvia', 'Lithuania', 'Mexico', 'Poland', 'Portugal', 'Slovenia', 'Spain')
Q3_b_data_for_italy <- Q3_b_data %>% filter(coded_country %in% clustered_countries)
Q3_b_data_non_italy <- Q3_b_data %>% filter(!coded_country %in% clustered_countries)
Q3_b_data_for_italy <- Q3_b_data_for_italy %>% select(-coded_country)
Q3_b_data_non_italy <- Q3_b_data_non_italy %>% select(-coded_country)
# Responses from Non-Italy people
Q3_b_italy_response <- Q3_b_data_for_italy[,-c(48:51)]
Q3_b_other_response <- Q3_b_data_non_italy[,-c(48:51)]
# C19, 4 columns for Non-Italy Q2-c
Q3_b_italy_c19 <- Q3_b_data_for_italy[48:51]
Q3_b_other_c19 <- Q3_b_data_non_italy[48:51]
Q3_italy_cor <- cor(Q3_b_italy_response, Q3_b_italy_c19)
Q3_other_cor <- cor(Q3_b_other_response, Q3_b_other_c19)
# Making the Data Frame
top_5_values_df_italy_q3 <- data.frame(Attribute = character(), Value = numeric(), stringsAsFactors = FALSE)
top_5_values_df_other_q3 <- data.frame(Attribute = character(), Value = numeric(), stringsAsFactors = FALSE)
# find the attribute that has the biggest absolute value of attribute for cor relationship, each row 7 attributes
for (col_index in 1:ncol(Q3_italy_cor)) {
col_values_italy <- Q3_italy_cor[, col_index]
abs_col_values <- abs(col_values_italy)
top_5_idx_italy <- order(abs_col_values, decreasing = TRUE)[1:7]
top_5_values_italy <- col_values_italy[top_5_idx_italy]
top_5_attribute_names_italy <- rownames(Q3_italy_cor)[top_5_idx_italy]
# save 7 for each row, exclude same attributes
for (i in 1:7) {
if (!(top_5_attribute_names_italy[i] %in% top_5_values_df_italy_q3$Attribute)) {
top_5_values_df_italy_q3 <- rbind(top_5_values_df_italy_q3, data.frame(Attribute = top_5_attribute_names_italy[i], Value = top_5_values_italy[i]))
}
}
}
# sort
top_5_values_df_italy_q3 <- top_5_values_df_italy_q3[order(-abs(top_5_values_df_italy_q3$Value)),]
# Making the Data Frame
top_5_values_df_other_q3 <- data.frame(Attribute = character(), Value = numeric(), stringsAsFactors = FALSE)
# find the attribute that has the biggest absolute value of attribute for cor relationship, each row 7 attributes
for (col_index in 1:ncol(Q3_other_cor)) {
col_values_other <- Q3_other_cor[, col_index]
abs_col_values <- abs(col_values_other)
top_5_idx_other <- order(abs_col_values, decreasing = TRUE)[1:7]
top_5_values_other <- col_values_other[top_5_idx_other]
top_5_attribute_names_other <- rownames(Q3_other_cor)[top_5_idx_other]
# save 7 for each row, exclude same attributes
for (i in 1:7) {
if (!(top_5_attribute_names_other[i] %in% top_5_values_df_other_q3$Attribute)) {
top_5_values_df_other_q3 <- rbind(top_5_values_df_other_q3, data.frame(Attribute = top_5_attribute_names_other[i], Value = top_5_values_other[i]))
}
}
}
# sort
top_5_values_df_other_q3 <- top_5_values_df_other_q3[order(-abs(top_5_values_df_other_q3$Value)),]
# get the colname of pro-social attribute
Q3_b_italy_c19 <- colnames(Q3_b_data_for_italy[48:51])
# extract pro_social data
pro_social_italy <- Q3_b_data_for_italy[, Q3_b_italy_c19]
# select the chosen attribute from the data
selected_attributes_italy <- top_5_values_df_italy_q3$Attribute
# calculate the correlation between pro-social attribute with data
corr_matrix_italy <- cor(Q3_b_data_for_italy[, selected_attributes_italy], pro_social_italy)
# reframe the data for heatmap
corr_melt_italy <- melt(corr_matrix_italy)
# heatmap
heatmap_sa_italy_3B <- ggplot(corr_melt_italy, aes(Var2, Var1)) +
geom_tile(aes(fill = value)) +
scale_fill_gradient2(low = "pink", high ="lightblue", mid = "white", midpoint = 0) +
theme(axis.text.x = element_text(size = 8),  # x 축의 레이블 크기 및 각도 조정
axis.text.y = element_text(size = 8),  # y 축의 레이블 크기 조정
plot.title = element_text(size = 15, hjust = 0.5)) +  # 그래프 제목 크기 조정
labs(title = "Correlation Heatmap for Merged Italy Cluster - Question 3B") +
xlab("pro-social attitudes") +
ylab("participant responses in Italy")
heatmap_sa_italy_3B
# get the colname of pro-social attribute
Q3_b_other_c19 <- colnames(Q3_b_data_non_italy[48:51])
# extract pro_social data
pro_social_other <- Q3_b_data_non_italy[, Q3_b_other_c19]
# select the chosen attribute from the data
selected_attributes_other <- top_5_values_df_other_q3$Attribute
# calculate the correlation between pro-social attribute with data
corr_matrix_other <- cor(Q3_b_data_non_italy[, selected_attributes_other], pro_social_other)
# reframe the data for heatmap
corr_melt_other <- melt(corr_matrix_other)
# heatmap
heatmap_sa_other_3B <- ggplot(corr_melt_other, aes(Var2, Var1)) +
geom_tile(aes(fill = value)) +
scale_fill_gradient2(low = "pink", high ="lightblue", mid = "white", midpoint = 0) +
theme(axis.text.x = element_text(size = 8),  # x 축의 레이블 크기 및 각도 조정
axis.text.y = element_text(size = 8),  # y 축의 레이블 크기 조정
plot.title = element_text(size = 15, hjust = 0.5)) +  # 그래프 제목 크기 조정
labs(title = "Correlation Heatmap for Merged Other - Question 3B") +
xlab("pro-social attitudes") +
ylab("participant responses in Non-Italy")
heatmap_sa_other_3B
top_5_values_df_other_q3$Attribute
top_5_values_df_italy_q3$Attribute
# Progression for making attribute format for lm()
predictors_formula_italy_q3 <- paste(top_5_values_df_other_q3$Attribute, collapse = " + ")
Q3_italy_so1 <- lm(paste("c19ProSo01 ~", predictors_formula_italy_q3, "+ c19ProSo02 + c19ProSo03 + c19ProSo04"), data = Q3_b_data_for_italy)
Q3_italy_so2 <- lm(paste("c19ProSo02 ~", predictors_formula_italy_q3, "+ c19ProSo01 + c19ProSo03 + c19ProSo04"), data = Q3_b_data_for_italy)
Q3_italy_so3 <- lm(paste("c19ProSo03 ~", predictors_formula_italy_q3, "+ c19ProSo01 + c19ProSo02 + c19ProSo04"), data = Q3_b_data_for_italy)
Q3_italy_so4 <- lm(paste("c19ProSo04 ~", predictors_formula_italy_q3, "+ c19ProSo01 + c19ProSo02 + c19ProSo03"), data = Q3_b_data_for_italy)
# Output
summary(Q3_italy_so1)
summary(Q3_italy_so2)
summary(Q3_italy_so3)
summary(Q3_italy_so4)
# Progression for making attribute format for lm()
predictors_formula_other_q3 <- paste(top_5_values_df_other_q3$Attribute, collapse = " + ")
Q3_other_so1 <- lm(paste("c19ProSo01 ~", predictors_formula_other_q3, "+ c19ProSo02 + c19ProSo03 + c19ProSo04"), data = Q3_b_data)
Q3_other_so2 <- lm(paste("c19ProSo02 ~", predictors_formula_other_q3, "+ c19ProSo01 + c19ProSo03 + c19ProSo04"), data = Q3_b_data)
Q3_other_so3 <- lm(paste("c19ProSo03 ~", predictors_formula_other_q3, "+ c19ProSo01 + c19ProSo02 + c19ProSo04"), data = Q3_b_data)
Q3_other_so4 <- lm(paste("c19ProSo04 ~", predictors_formula_other_q3, "+ c19ProSo01 + c19ProSo02 + c19ProSo03"), data = Q3_b_data)
# Output
summary(Q3_other_so1)
summary(Q3_other_so2)
summary(Q3_other_so3)
summary(Q3_other_so4)
cb <- Q3_a_data
cb[,2:7] = scale(cb[2:7])
rownames(cb) = cb$coded_country
cfit = hclust(dist(cb[,2:7]), "average")
cut.cfit = cutree(cfit,k=3)
# 클러스터링 결과를 시각화하기 위해 plot 함수를 사용합니다.
plot(cfit, main = "Dendrogram of Clustering", xlab = "Countries")
# 클러스터를 강조하기 위해 사각형 그리기
rect.hclust(cfit, k=3, border = "red")
install.packages("tree")
library(tree)
setwd("~/Documents/Monash University/FIT 3152 Data Analysis/Week 10")
install.packages("tm")
install.packages("SnowballC")
library(slam)
library(tm)
library(SnowballC)
cname=file.path(".","txt/")
cname
cname = file.path(".","CorpusAbstracts","txt")
cname
dir(cname)
cname = file.path(".","txt")
cname
dir(cname)
docs = Corpus(DirSource(cname))
summary(docs)
install.packages("proxy")
setwd("~/Documents/GitHub/Data-Analysis-Project-2")
knitr::opts_chunk$set(echo = TRUE)
Phish <- read.csv("PhishingData.csv")
set.seed(31994695) # Your Student ID is the random seed
L <- as.data.frame(c(1:50))
L <- L[sample(nrow(L), 10, replace = FALSE),]
Phish <- Phish[(Phish$A01 %in% L),]
PD <- Phish[sample(nrow(Phish), 2000, replace = FALSE),] # sample of 2000 rows
# sample of 2000 rows
head(Phish)
View(Phish)
# sample of 2000 rows
type(Phish)
# sample of 2000 rows
datatype(Phish)
# sample of 2000 rows
data.class(Phish)
# sample of 2000 rows
rownames(Phish)
Phish <- read.csv("PhishingData.csv")
Phish <- read.csv("PhishingData.csv")
set.seed(31994695) # Your Student ID is the random seed
L <- as.data.frame(c(1:50))
L <- L[sample(nrow(L), 10, replace = FALSE),]
Phish <- Phish[(Phish$A01 %in% L),]
Phish
L
Phish
L
Phish
Phish <- read.csv("PhishingData.csv")
Phish <- read.csv("PhishingData.csv")
set.seed(31994695) # Your Student ID is the random seed
set.seed(31994695) # Your Student ID is the random seed
L <- as.data.frame(c(1:50))
L <- L[sample(nrow(L), 10, replace = FALSE),]
Phish <- Phish[(Phish$A01 %in% L),]
PD <- Phish[sample(nrow(Phish), 2000, replace = FALSE),]
# sample of 2000 rows
head(PD)
# sample of 2000 rows
install.packages(XQuartz)
# sample of 2000 rows
install.packages("XQuartz")
install.packages("installr")
library(installr)
check.for.updates.R()
check.for.updates.R()
install.R()
version
check.for.updates.R()
library(installr)
check.for.updates.R()
updateR(GUI=FALSE)
updateR()
updateR()
updateR()
